{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_levels = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../pose-action/features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178.864</td>\n",
       "      <td>180.766</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>198.379</td>\n",
       "      <td>261.046</td>\n",
       "      <td>0.854602</td>\n",
       "      <td>131.697</td>\n",
       "      <td>270.835</td>\n",
       "      <td>0.764647</td>\n",
       "      <td>125.874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>739.041</td>\n",
       "      <td>147.424</td>\n",
       "      <td>0.908013</td>\n",
       "      <td>758.704</td>\n",
       "      <td>212.014</td>\n",
       "      <td>0.949156</td>\n",
       "      <td>715.641</td>\n",
       "      <td>210.162</td>\n",
       "      <td>0.872539</td>\n",
       "      <td>695.949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196.373</td>\n",
       "      <td>180.794</td>\n",
       "      <td>0.960075</td>\n",
       "      <td>206.199</td>\n",
       "      <td>261.029</td>\n",
       "      <td>0.837968</td>\n",
       "      <td>133.707</td>\n",
       "      <td>266.929</td>\n",
       "      <td>0.740769</td>\n",
       "      <td>120.070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196.522</td>\n",
       "      <td>182.778</td>\n",
       "      <td>0.885628</td>\n",
       "      <td>208.134</td>\n",
       "      <td>259.180</td>\n",
       "      <td>0.857302</td>\n",
       "      <td>133.798</td>\n",
       "      <td>263.053</td>\n",
       "      <td>0.737462</td>\n",
       "      <td>120.051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>694.069</td>\n",
       "      <td>196.346</td>\n",
       "      <td>0.955753</td>\n",
       "      <td>729.249</td>\n",
       "      <td>214.107</td>\n",
       "      <td>0.902440</td>\n",
       "      <td>686.212</td>\n",
       "      <td>217.955</td>\n",
       "      <td>0.793841</td>\n",
       "      <td>668.633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1         2        3        4         5        6        7  \\\n",
       "0  178.864  180.766  0.968300  198.379  261.046  0.854602  131.697  270.835   \n",
       "1  739.041  147.424  0.908013  758.704  212.014  0.949156  715.641  210.162   \n",
       "2  196.373  180.794  0.960075  206.199  261.029  0.837968  133.707  266.929   \n",
       "3  196.522  182.778  0.885628  208.134  259.180  0.857302  133.798  263.053   \n",
       "4  694.069  196.346  0.955753  729.249  214.107  0.902440  686.212  217.955   \n",
       "\n",
       "          8        9  ...  290  291  292  293  294  295  296  297  298  299  \n",
       "0  0.764647  125.874  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.872539  695.949  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.740769  120.070  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.737462  120.051  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.793841  668.633  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join(base_dir, 'pose_keypoints_with_labels.csv'))\n",
    "labels = (df['label'] <= 0).astype(int) #binarize labels\n",
    "df = df.iloc[:,:-1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6935 6935\n"
     ]
    }
   ],
   "source": [
    "print(len(df), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5491\n",
       "1    1444\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_p1 = range(0,24)\n",
    "r_p2 = range(75,99)\n",
    "r_p3 = range(150, 174)\n",
    "r_p4 = range(225,249)\n",
    "r_all = np.r_[r_p1, r_p2, r_p3, r_p4]\n",
    "\n",
    "df_p1 = df.iloc[:, r_p1]\n",
    "df_p2 = df.iloc[:, r_p2]\n",
    "df_p3 = df.iloc[:, r_p3]\n",
    "df_p4 = df.iloc[:, r_p4]\n",
    "df_all = df.iloc[:, r_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASABUNCUOGLU13\\AppData\\Local\\Temp\\ipykernel_9320\\2577857134.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p1['label'] = labels.values\n",
      "C:\\Users\\ASABUNCUOGLU13\\AppData\\Local\\Temp\\ipykernel_9320\\2577857134.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p2['label'] = labels.values\n",
      "C:\\Users\\ASABUNCUOGLU13\\AppData\\Local\\Temp\\ipykernel_9320\\2577857134.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p3['label'] = labels.values\n",
      "C:\\Users\\ASABUNCUOGLU13\\AppData\\Local\\Temp\\ipykernel_9320\\2577857134.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p4['label'] = labels.values\n",
      "C:\\Users\\ASABUNCUOGLU13\\AppData\\Local\\Temp\\ipykernel_9320\\2577857134.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all['label'] = labels.values\n"
     ]
    }
   ],
   "source": [
    "df_p1['label'] = labels.values\n",
    "df_p2['label'] = labels.values\n",
    "df_p3['label'] = labels.values\n",
    "df_p4['label'] = labels.values\n",
    "df_all['label'] = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"P1\": df_p1,\n",
    "    \"P2\": df_p2,\n",
    "    \"P3\": df_p3,\n",
    "    \"P4\": df_p4,\n",
    "    \"All Features\": df_all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/christophM/rulefit.git\n",
    "from rulefit import RuleFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuleFit(max_iter=1000, model_type='r', n_jobs=-1, rfmode='classify',\n",
      "        tree_generator=GradientBoostingClassifier(learning_rate=0.01,\n",
      "                                                  max_depth=100,\n",
      "                                                  max_leaf_nodes=2,\n",
      "                                                  n_estimators=550,\n",
      "                                                  random_state=549,\n",
      "                                                  subsample=0.09857775536929808)) f1 0.8184506891569311\n",
      "RuleFit(max_iter=1000, model_type='r', n_jobs=-1, rfmode='classify',\n",
      "        tree_generator=GradientBoostingClassifier(learning_rate=0.01,\n",
      "                                                  max_depth=100,\n",
      "                                                  max_leaf_nodes=3,\n",
      "                                                  n_estimators=567,\n",
      "                                                  random_state=566,\n",
      "                                                  subsample=0.09857775536929808)) f1 0.8080967814539559\n",
      "RuleFit(max_iter=1000, model_type='r', n_jobs=-1, rfmode='classify',\n",
      "        tree_generator=GradientBoostingClassifier(learning_rate=0.01,\n",
      "                                                  max_depth=100,\n",
      "                                                  max_leaf_nodes=9,\n",
      "                                                  n_estimators=556,\n",
      "                                                  random_state=555,\n",
      "                                                  subsample=0.09857775536929808)) f1 0.801881292594165\n",
      "RuleFit(max_iter=1000, model_type='r', n_jobs=-1, rfmode='classify',\n",
      "        tree_generator=GradientBoostingClassifier(learning_rate=0.01,\n",
      "                                                  max_depth=100,\n",
      "                                                  max_leaf_nodes=5,\n",
      "                                                  n_estimators=588,\n",
      "                                                  random_state=587,\n",
      "                                                  subsample=0.09857775536929808)) f1 0.8360663648295513\n",
      "RuleFit(max_iter=1000, model_type='r', n_jobs=-1, rfmode='classify',\n",
      "        tree_generator=GradientBoostingClassifier(learning_rate=0.01,\n",
      "                                                  max_depth=100,\n",
      "                                                  max_leaf_nodes=2,\n",
      "                                                  n_estimators=546,\n",
      "                                                  random_state=545,\n",
      "                                                  subsample=0.09857775536929808)) f1 0.8564249976297093\n"
     ]
    }
   ],
   "source": [
    "for title in feature_sets:\n",
    "    dfc = feature_sets[title]\n",
    "    not_zero_ind = ~(dfc == 0).all(axis=1)\n",
    "\n",
    "    dfc = dfc.loc[not_zero_ind]\n",
    "    labels = dfc['label'].loc[not_zero_ind]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_samples = scaler.fit_transform(dfc.iloc[:,:-1])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_samples, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    features = dfc.columns\n",
    "\n",
    "    rf = RuleFit(model_type='r', rfmode='classify', max_iter=5000, n_jobs=-1) ## Classification task with only rule-based (not linear) with all CPUs\n",
    "    rf.fit(X_train, y_train, feature_names=features)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    res = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(rf, \"f1\", res)\n",
    "    rules = rf.get_rules()\n",
    "    rules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n",
    "    rules.to_csv(\"reports/interpret/pose/rule-%s.csv\" % title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pca = PCA()\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('rf', rf)])\n",
    "\"\"\"\n",
    "blackbox_model = SVC(gamma=2, C=1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.perf import ROC\n",
    "from interpret.blackbox import LimeTabular\n",
    "from interpret import show\n",
    "from interpret.blackbox import ShapKernel\n",
    "from interpret.blackbox import MorrisSensitivity\n",
    "from interpret.blackbox import PartialDependence\n",
    "from interpret.glassbox import ExplainableBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASABUNCUOGLU13\\Anaconda3\\lib\\site-packages\\interpret\\visual\\udash.py:5: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n",
      "c:\\Users\\ASABUNCUOGLU13\\Anaconda3\\lib\\site-packages\\interpret\\visual\\udash.py:6: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "c:\\Users\\ASABUNCUOGLU13\\Anaconda3\\lib\\site-packages\\interpret\\visual\\udash.py:7: UserWarning: \n",
      "The dash_table package is deprecated. Please replace\n",
      "`import dash_table` with `from dash import dash_table`\n",
      "\n",
      "Also, if you're using any of the table format helpers (e.g. Group), replace \n",
      "`from dash_table.Format import Group` with \n",
      "`from dash.dash_table.Format import Group`\n",
      "  import dash_table as dt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2622473544992/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2622473544992/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2622543575792/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2622543575792/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2622470397232/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2622470397232/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2622526414272/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2622526414272/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2622473982736/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2622473982736/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for title in feature_sets:\n",
    "    ebm = ExplainableBoostingClassifier()\n",
    "    dfc = feature_sets[title]\n",
    "    not_zero_ind = ~(dfc == 0).all(axis=1)\n",
    "\n",
    "    dfc = dfc.loc[not_zero_ind]\n",
    "    labels = dfc['label'].loc[not_zero_ind]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_samples = scaler.fit_transform(dfc.iloc[:,:-1])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_samples, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    ebm.fit(X_train, y_train)\n",
    "    ebm_global = ebm.explain_global()\n",
    "    show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-size array to reduction operation maximum which has no identity\n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "for title in feature_sets:\n",
    "    dfc = feature_sets[title]\n",
    "    not_zero_ind = ~(dfc == 0).all(axis=1)\n",
    "\n",
    "    dfc = dfc.loc[not_zero_ind]\n",
    "    labels = dfc['label'].loc[not_zero_ind]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_samples = scaler.fit_transform(dfc.iloc[:,:-1])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_samples, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    blackbox_model.fit(X_train, y_train)\n",
    "    try:\n",
    "        sensitivity = MorrisSensitivity(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "        sensitivity_global = sensitivity.explain_global(name=\"Global Sensitivity\")\n",
    "\n",
    "        show(sensitivity_global)\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"zero-size array to reduction operation maximum which has no identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76be68d9c5d6e76a66d5315d11dc6a9ea46dedf1868770abac3c9563870c381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

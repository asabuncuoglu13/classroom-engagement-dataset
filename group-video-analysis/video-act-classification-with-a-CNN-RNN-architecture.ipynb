{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8x1ypYitnIdj"
   },
   "source": [
    "# Video Classification with a CNN-RNN Architecture\n",
    "\n",
    "This notebook is forked from [https://keras.io/examples/vision/video_classification/](Keras Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PKFTEBg2nIdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWPZV6SqnIdp"
   },
   "source": [
    "## Data collection\n",
    "\n",
    "In order to keep the runtime of this example relatively short, we will be using a\n",
    "subsampled version of the original UCF101 dataset. You can refer to\n",
    "[this notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb)\n",
    "to know how the subsampling was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VZXohs0rnIdq"
   },
   "outputs": [],
   "source": [
    "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
    "!tar xf ucf101_top5.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aAqIEMOnIdq"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zwgRbPR6nIdq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 14:34:12.304921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 14:34:12.752136: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-30 14:34:18.328359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /kuacc/apps/ffmpeg/4.4.0_x265/lib:/kuacc/apps/x265/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_iomp/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_tbb/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_dpcpp_gpu_dpcpp/lib:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib/x64:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib/emu:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/compiler/lib/intel64_lin:/kuacc/apps/cudnn/v8.1.1_CUDA_11.X/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib\n",
      "2023-01-30 14:34:18.331242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /kuacc/apps/ffmpeg/4.4.0_x265/lib:/kuacc/apps/x265/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_iomp/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_tbb/lib:/scratch/kuacc/apps/intel/oneapi/dnnl/2021.1.1/cpu_dpcpp_gpu_dpcpp/lib:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib/x64:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/lib/emu:/scratch/kuacc/apps/intel/oneapi/compiler/2021.1.1/linux/compiler/lib/intel64_lin:/kuacc/apps/cudnn/v8.1.1_CUDA_11.X/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib\n",
      "2023-01-30 14:34:18.331292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!export TF_ENABLE_ONEDNN_OPTS=1\n",
    "!export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 30 14:34:30 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:1C:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    26W /  70W |      0MiB / 15109MiB |      3%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoezlOllnIdr"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Y62ActoknIdr"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_checkpoint_path = \"./tmp/training_lstm/cp-{epoch:04d}.ckpt\"\n",
    "lstm_checkpoint_dir = os.path.dirname(lstm_checkpoint_path)\n",
    "\n",
    "transformer_checkpoint_path = \"./tmp/training_former/cp-{epoch:04d}.ckpt\"\n",
    "transformer_checkpoint_dir = os.path.dirname(transformer_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_btj6eWwnIds"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lxk8V5QinIds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 2465\n"
     ]
    }
   ],
   "source": [
    "basedir = \"../engagement-slices/%d\"\n",
    "df_vids = pd.read_csv(os.path.join(basedir % 4, \"list.csv\"))\n",
    "df_vids.insert(0, 'path', basedir)\n",
    "\n",
    "for i in range(5,9):\n",
    "    df = pd.read_csv(os.path.join(basedir % i, \"list.csv\"))\n",
    "    df.insert(0, 'path', basedir % i)\n",
    "    df_vids = pd.concat([df_vids, df], axis=0)\n",
    "\n",
    "print(f\"Total videos: {len(df_vids)}\")\n",
    "df_vids.sample(10)\n",
    "\n",
    "train_df, test_df = train_test_split(df_vids, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-bDI2eFnIdt"
   },
   "source": [
    "One of the many challenges of training video classifiers is figuring out a way to feed\n",
    "the videos to a network. [This blog post](https://blog.coast.ai/five-video-classification-methods-implemented-in-keras-and-tensorflow-99cad29cc0b5)\n",
    "discusses five such methods. Since a video is an ordered sequence of frames, we could\n",
    "just extract the frames and put them in a 3D tensor. But the number of frames may differ\n",
    "from video to video which would prevent us from stacking them into batches\n",
    "(unless we use padding). As an alternative, we can **save video frames at a fixed\n",
    "interval until a maximum frame count is reached**. In this example we will do\n",
    "the following:\n",
    "\n",
    "1. Capture the frames of a video.\n",
    "2. Extract frames from the videos until a maximum frame count is reached.\n",
    "3. In the case, where a video's frame count is lesser than the maximum frame count we\n",
    "will pad the video with zeros.\n",
    "\n",
    "Note that this workflow is identical to [problems involving texts sequences](https://developers.google.com/machine-learning/guides/text-classification/). Videos of the UCF101 dataset is [known](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)\n",
    "to not contain extreme variations in objects and actions across frames. Because of this,\n",
    "it may be okay to only consider a few frames for the learning task. But this approach may\n",
    "not generalize well to other video classification problems. We will be using\n",
    "[OpenCV's `VideoCapture()` method](https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html)\n",
    "to read frames from videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Ecngh3YXnIdt"
   },
   "outputs": [],
   "source": [
    "# The following two methods are taken from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m3e_Gf2nIdu"
   },
   "source": [
    "We can use a pre-trained network to extract meaningful features from the extracted\n",
    "frames. The [`Keras Applications`](https://keras.io/api/applications/) module provides\n",
    "a number of state-of-the-art models pre-trained on the [ImageNet-1k dataset](http://image-net.org/).\n",
    "We will be using the [InceptionV3 model](https://arxiv.org/abs/1512.00567) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2VGV-zvDnIdu"
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.MobileNet(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmUVuWX7nIdu"
   },
   "source": [
    "The labels of the videos are strings. Neural networks do not understand string values,\n",
    "so they must be converted to some numerical form before they are fed to the model. Here\n",
    "we will use the [`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\n",
    "layer encode the class labels as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7cQsOUTwnIdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'E', 'HD', 'HE', 'M']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(df_vids[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUKt7ZLEnIdv"
   },
   "source": [
    "Finally, we can put all the pieces together to create our data processing utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MeCd0exFnIdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (1972, 20, 2048)\n",
      "Frame masks in train set: (1972, 20)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df):\n",
    "    num_samples = len(df)\n",
    "    video_paths = (df['path'].astype(str) + df[\"video_name\"].astype(str)).values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df)\n",
    "test_data, test_labels = prepare_all_videos(test_df)\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7M_GI_QnIdv"
   },
   "source": [
    "The above code block will take ~20 minutes to execute depending on the machine it's being\n",
    "executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UUrHhE4nIdw"
   },
   "source": [
    "## The sequence model\n",
    "\n",
    "Now, we can feed this data to a sequence model consisting of recurrent layers like `GRU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "04lUzuafnIdw"
   },
   "outputs": [],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=lstm_checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     save_best_only=True,\n",
    "                                                     verbose=1)\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5996 - accuracy: 0.3228\n",
      "Epoch 1: val_loss improved from inf to 1.58958, saving model to ./tmp/training_lstm/cp-0001.ckpt\n",
      "50/50 [==============================] - 15s 136ms/step - loss: 1.5996 - accuracy: 0.3228 - val_loss: 1.5896 - val_accuracy: 0.3468\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5795 - accuracy: 0.3748\n",
      "Epoch 2: val_loss improved from 1.58958 to 1.57203, saving model to ./tmp/training_lstm/cp-0002.ckpt\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 1.5795 - accuracy: 0.3748 - val_loss: 1.5720 - val_accuracy: 0.3468\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5614 - accuracy: 0.3748\n",
      "Epoch 3: val_loss improved from 1.57203 to 1.55597, saving model to ./tmp/training_lstm/cp-0003.ckpt\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 1.5614 - accuracy: 0.3748 - val_loss: 1.5560 - val_accuracy: 0.3468\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5455 - accuracy: 0.3748\n",
      "Epoch 4: val_loss improved from 1.55597 to 1.54167, saving model to ./tmp/training_lstm/cp-0004.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.5455 - accuracy: 0.3748 - val_loss: 1.5417 - val_accuracy: 0.3468\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5311 - accuracy: 0.3748\n",
      "Epoch 5: val_loss improved from 1.54167 to 1.52925, saving model to ./tmp/training_lstm/cp-0005.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.5311 - accuracy: 0.3748 - val_loss: 1.5292 - val_accuracy: 0.3468\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5180 - accuracy: 0.3748\n",
      "Epoch 6: val_loss improved from 1.52925 to 1.51776, saving model to ./tmp/training_lstm/cp-0006.ckpt\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 1.5180 - accuracy: 0.3748 - val_loss: 1.5178 - val_accuracy: 0.3468\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5063 - accuracy: 0.3748\n",
      "Epoch 7: val_loss improved from 1.51776 to 1.50773, saving model to ./tmp/training_lstm/cp-0007.ckpt\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 1.5063 - accuracy: 0.3748 - val_loss: 1.5077 - val_accuracy: 0.3468\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4959 - accuracy: 0.3748\n",
      "Epoch 8: val_loss improved from 1.50773 to 1.49851, saving model to ./tmp/training_lstm/cp-0008.ckpt\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 1.4959 - accuracy: 0.3748 - val_loss: 1.4985 - val_accuracy: 0.3468\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4865 - accuracy: 0.3748\n",
      "Epoch 9: val_loss improved from 1.49851 to 1.49079, saving model to ./tmp/training_lstm/cp-0009.ckpt\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 1.4865 - accuracy: 0.3748 - val_loss: 1.4908 - val_accuracy: 0.3468\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4781 - accuracy: 0.3748\n",
      "Epoch 10: val_loss improved from 1.49079 to 1.48362, saving model to ./tmp/training_lstm/cp-0010.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4781 - accuracy: 0.3748 - val_loss: 1.4836 - val_accuracy: 0.3468\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4708 - accuracy: 0.3748\n",
      "Epoch 11: val_loss improved from 1.48362 to 1.47737, saving model to ./tmp/training_lstm/cp-0011.ckpt\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 1.4708 - accuracy: 0.3748 - val_loss: 1.4774 - val_accuracy: 0.3468\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4642 - accuracy: 0.3748\n",
      "Epoch 12: val_loss improved from 1.47737 to 1.47203, saving model to ./tmp/training_lstm/cp-0012.ckpt\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 1.4642 - accuracy: 0.3748 - val_loss: 1.4720 - val_accuracy: 0.3468\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4583 - accuracy: 0.3748\n",
      "Epoch 13: val_loss improved from 1.47203 to 1.46683, saving model to ./tmp/training_lstm/cp-0013.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4583 - accuracy: 0.3748 - val_loss: 1.4668 - val_accuracy: 0.3468\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4530 - accuracy: 0.3748\n",
      "Epoch 14: val_loss improved from 1.46683 to 1.46246, saving model to ./tmp/training_lstm/cp-0014.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4530 - accuracy: 0.3748 - val_loss: 1.4625 - val_accuracy: 0.3468\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4482 - accuracy: 0.3748\n",
      "Epoch 15: val_loss improved from 1.46246 to 1.45859, saving model to ./tmp/training_lstm/cp-0015.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4482 - accuracy: 0.3748 - val_loss: 1.4586 - val_accuracy: 0.3468\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4440 - accuracy: 0.3748\n",
      "Epoch 16: val_loss improved from 1.45859 to 1.45490, saving model to ./tmp/training_lstm/cp-0016.ckpt\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 1.4440 - accuracy: 0.3748 - val_loss: 1.4549 - val_accuracy: 0.3468\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4401 - accuracy: 0.3748\n",
      "Epoch 17: val_loss improved from 1.45490 to 1.45174, saving model to ./tmp/training_lstm/cp-0017.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4401 - accuracy: 0.3748 - val_loss: 1.4517 - val_accuracy: 0.3468\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4368 - accuracy: 0.3748\n",
      "Epoch 18: val_loss improved from 1.45174 to 1.44883, saving model to ./tmp/training_lstm/cp-0018.ckpt\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 1.4368 - accuracy: 0.3748 - val_loss: 1.4488 - val_accuracy: 0.3468\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4337 - accuracy: 0.3748\n",
      "Epoch 19: val_loss improved from 1.44883 to 1.44638, saving model to ./tmp/training_lstm/cp-0019.ckpt\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 1.4337 - accuracy: 0.3748 - val_loss: 1.4464 - val_accuracy: 0.3468\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4309 - accuracy: 0.3748\n",
      "Epoch 20: val_loss improved from 1.44638 to 1.44402, saving model to ./tmp/training_lstm/cp-0020.ckpt\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 1.4309 - accuracy: 0.3748 - val_loss: 1.4440 - val_accuracy: 0.3468\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4284 - accuracy: 0.3748\n",
      "Epoch 21: val_loss improved from 1.44402 to 1.44180, saving model to ./tmp/training_lstm/cp-0021.ckpt\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 1.4284 - accuracy: 0.3748 - val_loss: 1.4418 - val_accuracy: 0.3468\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4260 - accuracy: 0.3748\n",
      "Epoch 22: val_loss improved from 1.44180 to 1.43979, saving model to ./tmp/training_lstm/cp-0022.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4260 - accuracy: 0.3748 - val_loss: 1.4398 - val_accuracy: 0.3468\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4240 - accuracy: 0.3748\n",
      "Epoch 23: val_loss improved from 1.43979 to 1.43816, saving model to ./tmp/training_lstm/cp-0023.ckpt\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 1.4240 - accuracy: 0.3748 - val_loss: 1.4382 - val_accuracy: 0.3468\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4221 - accuracy: 0.3748\n",
      "Epoch 24: val_loss improved from 1.43816 to 1.43637, saving model to ./tmp/training_lstm/cp-0024.ckpt\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 1.4221 - accuracy: 0.3748 - val_loss: 1.4364 - val_accuracy: 0.3468\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4204 - accuracy: 0.3748\n",
      "Epoch 25: val_loss improved from 1.43637 to 1.43498, saving model to ./tmp/training_lstm/cp-0025.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4204 - accuracy: 0.3748 - val_loss: 1.4350 - val_accuracy: 0.3468\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 1.4188 - accuracy: 0.3748\n",
      "Epoch 26: val_loss improved from 1.43498 to 1.43342, saving model to ./tmp/training_lstm/cp-0026.ckpt\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 1.4188 - accuracy: 0.3748 - val_loss: 1.4334 - val_accuracy: 0.3468\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4173 - accuracy: 0.3748\n",
      "Epoch 27: val_loss improved from 1.43342 to 1.43216, saving model to ./tmp/training_lstm/cp-0027.ckpt\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 1.4173 - accuracy: 0.3748 - val_loss: 1.4322 - val_accuracy: 0.3468\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4160 - accuracy: 0.3748\n",
      "Epoch 28: val_loss improved from 1.43216 to 1.43074, saving model to ./tmp/training_lstm/cp-0028.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4160 - accuracy: 0.3748 - val_loss: 1.4307 - val_accuracy: 0.3468\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4148 - accuracy: 0.3748\n",
      "Epoch 29: val_loss improved from 1.43074 to 1.42960, saving model to ./tmp/training_lstm/cp-0029.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4148 - accuracy: 0.3748 - val_loss: 1.4296 - val_accuracy: 0.3468\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4136 - accuracy: 0.3748\n",
      "Epoch 30: val_loss improved from 1.42960 to 1.42875, saving model to ./tmp/training_lstm/cp-0030.ckpt\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 1.4136 - accuracy: 0.3748 - val_loss: 1.4287 - val_accuracy: 0.3468\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4125 - accuracy: 0.3748\n",
      "Epoch 31: val_loss improved from 1.42875 to 1.42759, saving model to ./tmp/training_lstm/cp-0031.ckpt\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 1.4125 - accuracy: 0.3748 - val_loss: 1.4276 - val_accuracy: 0.3468\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4115 - accuracy: 0.3748\n",
      "Epoch 32: val_loss improved from 1.42759 to 1.42668, saving model to ./tmp/training_lstm/cp-0032.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4115 - accuracy: 0.3748 - val_loss: 1.4267 - val_accuracy: 0.3468\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4106 - accuracy: 0.3748\n",
      "Epoch 33: val_loss improved from 1.42668 to 1.42583, saving model to ./tmp/training_lstm/cp-0033.ckpt\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 1.4106 - accuracy: 0.3748 - val_loss: 1.4258 - val_accuracy: 0.3468\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4097 - accuracy: 0.3748\n",
      "Epoch 34: val_loss improved from 1.42583 to 1.42482, saving model to ./tmp/training_lstm/cp-0034.ckpt\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 1.4097 - accuracy: 0.3748 - val_loss: 1.4248 - val_accuracy: 0.3468\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4091 - accuracy: 0.3748\n",
      "Epoch 35: val_loss improved from 1.42482 to 1.42413, saving model to ./tmp/training_lstm/cp-0035.ckpt\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 1.4091 - accuracy: 0.3748 - val_loss: 1.4241 - val_accuracy: 0.3468\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4083 - accuracy: 0.3748\n",
      "Epoch 36: val_loss improved from 1.42413 to 1.42355, saving model to ./tmp/training_lstm/cp-0036.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4083 - accuracy: 0.3748 - val_loss: 1.4236 - val_accuracy: 0.3468\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4076 - accuracy: 0.3748\n",
      "Epoch 37: val_loss improved from 1.42355 to 1.42276, saving model to ./tmp/training_lstm/cp-0037.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4076 - accuracy: 0.3748 - val_loss: 1.4228 - val_accuracy: 0.3468\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4070 - accuracy: 0.3748\n",
      "Epoch 38: val_loss improved from 1.42276 to 1.42186, saving model to ./tmp/training_lstm/cp-0038.ckpt\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 1.4070 - accuracy: 0.3748 - val_loss: 1.4219 - val_accuracy: 0.3468\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4064 - accuracy: 0.3748\n",
      "Epoch 39: val_loss improved from 1.42186 to 1.42128, saving model to ./tmp/training_lstm/cp-0039.ckpt\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 1.4064 - accuracy: 0.3748 - val_loss: 1.4213 - val_accuracy: 0.3468\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4058 - accuracy: 0.3748\n",
      "Epoch 40: val_loss improved from 1.42128 to 1.42082, saving model to ./tmp/training_lstm/cp-0040.ckpt\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 1.4058 - accuracy: 0.3748 - val_loss: 1.4208 - val_accuracy: 0.3468\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4053 - accuracy: 0.3748\n",
      "Epoch 41: val_loss improved from 1.42082 to 1.42033, saving model to ./tmp/training_lstm/cp-0041.ckpt\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 1.4053 - accuracy: 0.3748 - val_loss: 1.4203 - val_accuracy: 0.3468\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4048 - accuracy: 0.3748\n",
      "Epoch 42: val_loss improved from 1.42033 to 1.41965, saving model to ./tmp/training_lstm/cp-0042.ckpt\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 1.4048 - accuracy: 0.3748 - val_loss: 1.4196 - val_accuracy: 0.3468\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4044 - accuracy: 0.3748\n",
      "Epoch 43: val_loss improved from 1.41965 to 1.41934, saving model to ./tmp/training_lstm/cp-0043.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4044 - accuracy: 0.3748 - val_loss: 1.4193 - val_accuracy: 0.3468\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4040 - accuracy: 0.3748\n",
      "Epoch 44: val_loss improved from 1.41934 to 1.41907, saving model to ./tmp/training_lstm/cp-0044.ckpt\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 1.4040 - accuracy: 0.3748 - val_loss: 1.4191 - val_accuracy: 0.3468\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4036 - accuracy: 0.3748\n",
      "Epoch 45: val_loss improved from 1.41907 to 1.41842, saving model to ./tmp/training_lstm/cp-0045.ckpt\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 1.4036 - accuracy: 0.3748 - val_loss: 1.4184 - val_accuracy: 0.3468\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4033 - accuracy: 0.3748\n",
      "Epoch 46: val_loss improved from 1.41842 to 1.41805, saving model to ./tmp/training_lstm/cp-0046.ckpt\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 1.4033 - accuracy: 0.3748 - val_loss: 1.4181 - val_accuracy: 0.3468\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4029 - accuracy: 0.3748\n",
      "Epoch 47: val_loss improved from 1.41805 to 1.41758, saving model to ./tmp/training_lstm/cp-0047.ckpt\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 1.4029 - accuracy: 0.3748 - val_loss: 1.4176 - val_accuracy: 0.3468\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4026 - accuracy: 0.3748\n",
      "Epoch 48: val_loss improved from 1.41758 to 1.41739, saving model to ./tmp/training_lstm/cp-0048.ckpt\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 1.4026 - accuracy: 0.3748 - val_loss: 1.4174 - val_accuracy: 0.3468\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4023 - accuracy: 0.3748\n",
      "Epoch 49: val_loss improved from 1.41739 to 1.41706, saving model to ./tmp/training_lstm/cp-0049.ckpt\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 1.4023 - accuracy: 0.3748 - val_loss: 1.4171 - val_accuracy: 0.3468\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4021 - accuracy: 0.3748\n",
      "Epoch 50: val_loss improved from 1.41706 to 1.41670, saving model to ./tmp/training_lstm/cp-0050.ckpt\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 1.4021 - accuracy: 0.3748 - val_loss: 1.4167 - val_accuracy: 0.3468\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, sequence_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 41\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m seq_model \u001b[38;5;241m=\u001b[39m get_sequence_model()\n\u001b[1;32m     33\u001b[0m history \u001b[38;5;241m=\u001b[39m seq_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     34\u001b[0m     [train_data[\u001b[38;5;241m0\u001b[39m], train_data[\u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     35\u001b[0m     train_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m seq_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[43mfilepath\u001b[49m)\n\u001b[1;32m     42\u001b[0m _, accuracy \u001b[38;5;241m=\u001b[39m seq_model\u001b[38;5;241m.\u001b[39mevaluate([test_data[\u001b[38;5;241m0\u001b[39m], test_data[\u001b[38;5;241m1\u001b[39m]], test_labels)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utZWDczPnIdw"
   },
   "source": [
    "**Note**: To keep the runtime of this example relatively short, we just used a few\n",
    "training examples. This number of training examples is low with respect to the sequence\n",
    "model being used that has 99,909 trainable parameters. You are encouraged to sample more\n",
    "data from the UCF101 dataset using [the notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb) mentioned above and train the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = len(label_processor.get_vocabulary())\n",
    "\n",
    "    inputs = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=transformer_checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     save_best_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "    model = get_compiled_model()\n",
    "    history = model.fit(\n",
    "        train_data[0],\n",
    "        train_labels,\n",
    "        validation_split=0.15,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    model.load_weights(transformer_checkpoint_path)\n",
    "    _, accuracy = model.evaluate(test_data[0], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 15:00:14.138499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - ETA: 0s - loss: 4.3808 - accuracy: 0.2613\n",
      "Epoch 1: val_loss improved from inf to 1.83581, saving model to ./tmp/training_former/cp-0001.ckpt\n",
      "53/53 [==============================] - 9s 50ms/step - loss: 4.3808 - accuracy: 0.2613 - val_loss: 1.8358 - val_accuracy: 0.3243\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.8575 - accuracy: 0.3037\n",
      "Epoch 2: val_loss improved from 1.83581 to 1.47224, saving model to ./tmp/training_former/cp-0002.ckpt\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 1.8575 - accuracy: 0.3037 - val_loss: 1.4722 - val_accuracy: 0.3243\n",
      "Epoch 3/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.7574 - accuracy: 0.2945\n",
      "Epoch 3: val_loss did not improve from 1.47224\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.7591 - accuracy: 0.2930 - val_loss: 1.7050 - val_accuracy: 0.1655\n",
      "Epoch 4/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.7006 - accuracy: 0.3113\n",
      "Epoch 4: val_loss improved from 1.47224 to 1.42628, saving model to ./tmp/training_former/cp-0004.ckpt\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 1.6957 - accuracy: 0.3103 - val_loss: 1.4263 - val_accuracy: 0.3243\n",
      "Epoch 5/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.5922 - accuracy: 0.3131\n",
      "Epoch 5: val_loss did not improve from 1.42628\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.5932 - accuracy: 0.3126 - val_loss: 1.4732 - val_accuracy: 0.3243\n",
      "Epoch 6/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.5921 - accuracy: 0.3251\n",
      "Epoch 6: val_loss improved from 1.42628 to 1.41985, saving model to ./tmp/training_former/cp-0006.ckpt\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 1.5914 - accuracy: 0.3264 - val_loss: 1.4198 - val_accuracy: 0.3243\n",
      "Epoch 7/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.5583 - accuracy: 0.3125\n",
      "Epoch 7: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.5580 - accuracy: 0.3132 - val_loss: 1.4794 - val_accuracy: 0.3243\n",
      "Epoch 8/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.5351 - accuracy: 0.3065\n",
      "Epoch 8: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.5332 - accuracy: 0.3079 - val_loss: 1.6403 - val_accuracy: 0.3243\n",
      "Epoch 9/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.5511 - accuracy: 0.3239\n",
      "Epoch 9: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 1.5493 - accuracy: 0.3240 - val_loss: 1.4559 - val_accuracy: 0.3243\n",
      "Epoch 10/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4916 - accuracy: 0.3425\n",
      "Epoch 10: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4926 - accuracy: 0.3413 - val_loss: 1.4295 - val_accuracy: 0.3243\n",
      "Epoch 11/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.5116 - accuracy: 0.3272\n",
      "Epoch 11: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.5144 - accuracy: 0.3264 - val_loss: 1.4879 - val_accuracy: 0.3243\n",
      "Epoch 12/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4774 - accuracy: 0.3257\n",
      "Epoch 12: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4782 - accuracy: 0.3252 - val_loss: 1.4372 - val_accuracy: 0.3209\n",
      "Epoch 13/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4877 - accuracy: 0.3245\n",
      "Epoch 13: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4877 - accuracy: 0.3252 - val_loss: 1.4303 - val_accuracy: 0.3243\n",
      "Epoch 14/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4733 - accuracy: 0.3335\n",
      "Epoch 14: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4746 - accuracy: 0.3329 - val_loss: 1.4995 - val_accuracy: 0.3209\n",
      "Epoch 15/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4690 - accuracy: 0.3329\n",
      "Epoch 15: val_loss did not improve from 1.41985\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4668 - accuracy: 0.3323 - val_loss: 1.4274 - val_accuracy: 0.3209\n",
      "Epoch 16/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4470 - accuracy: 0.3493\n",
      "Epoch 16: val_loss improved from 1.41985 to 1.41234, saving model to ./tmp/training_former/cp-0016.ckpt\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 1.4435 - accuracy: 0.3502 - val_loss: 1.4123 - val_accuracy: 0.3243\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4436 - accuracy: 0.3467\n",
      "Epoch 17: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4436 - accuracy: 0.3467 - val_loss: 1.4266 - val_accuracy: 0.3243\n",
      "Epoch 18/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4668 - accuracy: 0.3290\n",
      "Epoch 18: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4670 - accuracy: 0.3258 - val_loss: 1.4576 - val_accuracy: 0.3243\n",
      "Epoch 19/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4560 - accuracy: 0.3450\n",
      "Epoch 19: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4549 - accuracy: 0.3455 - val_loss: 1.4858 - val_accuracy: 0.3243\n",
      "Epoch 20/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4364 - accuracy: 0.3594\n",
      "Epoch 20: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4395 - accuracy: 0.3580 - val_loss: 1.4546 - val_accuracy: 0.3243\n",
      "Epoch 21/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4487 - accuracy: 0.3510\n",
      "Epoch 21: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4481 - accuracy: 0.3514 - val_loss: 1.4507 - val_accuracy: 0.3243\n",
      "Epoch 22/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4468 - accuracy: 0.3333\n",
      "Epoch 22: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4465 - accuracy: 0.3335 - val_loss: 1.4410 - val_accuracy: 0.3243\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4364 - accuracy: 0.3556\n",
      "Epoch 23: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4364 - accuracy: 0.3556 - val_loss: 1.4388 - val_accuracy: 0.3243\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4727 - accuracy: 0.3490\n",
      "Epoch 24: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4727 - accuracy: 0.3490 - val_loss: 1.4169 - val_accuracy: 0.3243\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4887 - accuracy: 0.3335\n",
      "Epoch 25: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4887 - accuracy: 0.3335 - val_loss: 1.4280 - val_accuracy: 0.3243\n",
      "Epoch 26/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4718 - accuracy: 0.3401\n",
      "Epoch 26: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4669 - accuracy: 0.3425 - val_loss: 1.4159 - val_accuracy: 0.3243\n",
      "Epoch 27/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4433 - accuracy: 0.3444\n",
      "Epoch 27: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4468 - accuracy: 0.3413 - val_loss: 1.4449 - val_accuracy: 0.3243\n",
      "Epoch 28/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4764 - accuracy: 0.3309\n",
      "Epoch 28: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4765 - accuracy: 0.3311 - val_loss: 1.4610 - val_accuracy: 0.3243\n",
      "Epoch 29/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4587 - accuracy: 0.3425\n",
      "Epoch 29: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4571 - accuracy: 0.3443 - val_loss: 1.4317 - val_accuracy: 0.3243\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4502 - accuracy: 0.3615\n",
      "Epoch 30: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4486 - accuracy: 0.3634 - val_loss: 1.4186 - val_accuracy: 0.3243\n",
      "Epoch 31/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4484 - accuracy: 0.3609\n",
      "Epoch 31: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4518 - accuracy: 0.3598 - val_loss: 1.4180 - val_accuracy: 0.3243\n",
      "Epoch 32/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4312 - accuracy: 0.3725\n",
      "Epoch 32: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 1.4312 - accuracy: 0.3735 - val_loss: 1.4521 - val_accuracy: 0.3243\n",
      "Epoch 33/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4293 - accuracy: 0.3615\n",
      "Epoch 33: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4287 - accuracy: 0.3604 - val_loss: 1.4272 - val_accuracy: 0.3243\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4085 - accuracy: 0.3568\n",
      "Epoch 34: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4085 - accuracy: 0.3568 - val_loss: 1.4284 - val_accuracy: 0.3209\n",
      "Epoch 35/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4125 - accuracy: 0.3572\n",
      "Epoch 35: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4145 - accuracy: 0.3562 - val_loss: 1.4366 - val_accuracy: 0.3243\n",
      "Epoch 36/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4862 - accuracy: 0.3499\n",
      "Epoch 36: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4842 - accuracy: 0.3526 - val_loss: 1.4194 - val_accuracy: 0.3243\n",
      "Epoch 37/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4449 - accuracy: 0.3474\n",
      "Epoch 37: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4455 - accuracy: 0.3461 - val_loss: 1.4304 - val_accuracy: 0.3243\n",
      "Epoch 38/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4166 - accuracy: 0.3652\n",
      "Epoch 38: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4178 - accuracy: 0.3663 - val_loss: 1.4325 - val_accuracy: 0.3243\n",
      "Epoch 39/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4161 - accuracy: 0.3600\n",
      "Epoch 39: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4179 - accuracy: 0.3604 - val_loss: 1.4434 - val_accuracy: 0.3243\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4253 - accuracy: 0.3508\n",
      "Epoch 40: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4253 - accuracy: 0.3508 - val_loss: 1.4569 - val_accuracy: 0.3243\n",
      "Epoch 41/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4452 - accuracy: 0.3388\n",
      "Epoch 41: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 1.4434 - accuracy: 0.3401 - val_loss: 1.4488 - val_accuracy: 0.3243\n",
      "Epoch 42/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4302 - accuracy: 0.3572\n",
      "Epoch 42: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4305 - accuracy: 0.3568 - val_loss: 1.4446 - val_accuracy: 0.3243\n",
      "Epoch 43/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4429 - accuracy: 0.3572\n",
      "Epoch 43: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4441 - accuracy: 0.3574 - val_loss: 1.4355 - val_accuracy: 0.3243\n",
      "Epoch 44/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4490 - accuracy: 0.3612\n",
      "Epoch 44: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 1.4494 - accuracy: 0.3610 - val_loss: 1.4156 - val_accuracy: 0.3209\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4448 - accuracy: 0.3604\n",
      "Epoch 45: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4448 - accuracy: 0.3604 - val_loss: 1.4220 - val_accuracy: 0.3243\n",
      "Epoch 46/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4482 - accuracy: 0.3505\n",
      "Epoch 46: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4479 - accuracy: 0.3496 - val_loss: 1.4597 - val_accuracy: 0.3243\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4383 - accuracy: 0.3455\n",
      "Epoch 47: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 1.4383 - accuracy: 0.3455 - val_loss: 1.4175 - val_accuracy: 0.3209\n",
      "Epoch 48/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.4391 - accuracy: 0.3504\n",
      "Epoch 48: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4383 - accuracy: 0.3508 - val_loss: 1.4653 - val_accuracy: 0.3243\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.4160 - accuracy: 0.3687\n",
      "Epoch 49: val_loss did not improve from 1.41234\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 1.4160 - accuracy: 0.3687 - val_loss: 1.4332 - val_accuracy: 0.3243\n",
      "Epoch 50/50\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 1.4235 - accuracy: 0.3523\n",
      "Epoch 50: val_loss improved from 1.41234 to 1.41126, saving model to ./tmp/training_former/cp-0050.ckpt\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.4244 - accuracy: 0.3490 - val_loss: 1.4113 - val_accuracy: 0.3209\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./tmp/training_former/cp-{epoch:04d}.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sequence_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 39\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m get_compiled_model()\n\u001b[1;32m     31\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     32\u001b[0m     train_data[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     33\u001b[0m     train_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m _, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_data[\u001b[38;5;241m0\u001b[39m], test_labels)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     27\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./tmp/training_former/cp-{epoch:04d}.ckpt"
     ]
    }
   ],
   "source": [
    "sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence_model = get_sequence_model().load_weights(lstm_checkpoint_path)\n",
    "sequence_model = model.load_weights(transformer_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TNJ8NgLnIdw"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nTDYowYnIdx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "    imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
    "    return embed.embed_file('./animation.gif')\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd2gROadnIdx"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* In this example, we made use of transfer learning for extracting meaningful features\n",
    "from video frames. You could also fine-tune the pre-trained network to notice how that\n",
    "affects the end results.\n",
    "* For speed-accuracy trade-offs, you can try out other models present inside\n",
    "`tf.keras.applications`.\n",
    "* Try different combinations of `MAX_SEQ_LENGTH` to observe how that affects the\n",
    "performance.\n",
    "* Train on a higher number of classes and see if you are able to get good performance.\n",
    "* Following [this tutorial](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub), try a\n",
    "[pre-trained action recognition model](https://arxiv.org/abs/1705.07750) from DeepMind.\n",
    "* Rolling-averaging can be useful technique for video classification and it can be\n",
    "combined with a standard image classification model to infer on videos.\n",
    "[This tutorial](https://www.pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/)\n",
    "will help understand how to use rolling-averaging with an image classifier.\n",
    "* When there are variations in between the frames of a video not all the frames might be\n",
    "equally important to decide its category. In those situations, putting a\n",
    "[self-attention layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) in the\n",
    "sequence model will likely yield better results.\n",
    "* Following [this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11),\n",
    "you can implement Transformers-based models for processing videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "video_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f76be68d9c5d6e76a66d5315d11dc6a9ea46dedf1868770abac3c9563870c381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
